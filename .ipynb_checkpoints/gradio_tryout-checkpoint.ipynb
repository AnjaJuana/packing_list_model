{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e25090fa-f990-4f1a-84f3-b12159eedae8",
   "metadata": {},
   "source": [
    "# Try out gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd23321-1870-44af-82ed-bb241d055dfa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bbee2e4-55c8-4b06-9929-72026edf7932",
   "metadata": {},
   "source": [
    "**Load prerequisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8c28d2d-8458-49fd-8ebf-5e729d6e861f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First trip: I am planning a trip to Greece with my boyfriend, where we will visit two islands. We have booked an apartment on each island for a few days and plan to spend most of our time relaxing. Our main goals are to enjoy the beach, try delicious local food, and possibly go on a hike—if it’s not too hot. We will be relying solely on public transport. We’re in our late 20s and traveling from the Netherlands. \n",
      "\n",
      "Trip type: ['beach vacation', ['swimming', 'going to the beach', 'relaxing', 'hiking'], 'warm destination / summer', 'lightweight (but comfortable)', 'casual', 'indoor', 'no own vehicle', 'no special conditions', '7+ days']\n"
     ]
    }
   ],
   "source": [
    "# Prerequisites\n",
    "from tabulate import tabulate\n",
    "from transformers import pipeline\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "# get candidate labels\n",
    "with open(\"packing_label_structure.json\", \"r\") as file:\n",
    "    candidate_labels = json.load(file)\n",
    "keys_list = list(candidate_labels.keys())\n",
    "\n",
    "# Load test data (in list of dictionaries)\n",
    "with open(\"test_data.json\", \"r\") as file:\n",
    "    packing_data = json.load(file)\n",
    "# Extract all trip descriptions and trip_types\n",
    "trip_descriptions = [trip['description'] for trip in packing_data]\n",
    "trip_types = [trip['trip_types'] for trip in packing_data]\n",
    "\n",
    "# Access the first trip description\n",
    "first_trip = trip_descriptions[0]\n",
    "# Get the packing list for the secondfirst trip\n",
    "first_trip_type = trip_types[0]\n",
    "\n",
    "print(f\"First trip: {first_trip} \\n\")\n",
    "print(f\"Trip type: {first_trip_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fac51224-9575-4b4b-8567-4ad4e759ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns pandas data frame with predictions\n",
    "\n",
    "cut_off = 0.5  # used to choose which activities are relevant\n",
    "\n",
    "def pred_trip(model_name, trip_descr, trip_type, cut_off):\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=model_name)\n",
    "    # Create an empty DataFrame with specified columns\n",
    "    df = pd.DataFrame(columns=['superclass', 'pred_class'])\n",
    "    for i, key in enumerate(keys_list):\n",
    "        if key == 'activities':\n",
    "            result = classifier(trip_descr, candidate_labels[key], multi_label=True)\n",
    "            indices = [i for i, score in enumerate(result['scores']) if score > cut_off]\n",
    "            classes = [result['labels'][i] for i in indices]\n",
    "        else:\n",
    "            result = classifier(trip_descr, candidate_labels[key])\n",
    "            classes = result[\"labels\"][0]\n",
    "        print(result)\n",
    "        print(classes)\n",
    "        print(i)\n",
    "        df.loc[i] = [key, classes]\n",
    "    df['true_class'] = trip_type\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b36ab806-2f35-4950-ac5a-7c192190cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for accuracy, perc true classes identified and perc wrong pred classes\n",
    "\n",
    "def perf_measure(df):\n",
    "    df['same_value'] = df['pred_class'] == df['true_class']\n",
    "    correct = sum(df.loc[df.index != 1, 'same_value'])\n",
    "    total = len(df['same_value'])\n",
    "    accuracy = correct/total\n",
    "    pred_class = df.loc[df.index == 1, 'pred_class'].iloc[0]\n",
    "    true_class = df.loc[df.index == 1, 'true_class'].iloc[0]\n",
    "    correct = [label for label in pred_class if label in true_class]\n",
    "    num_correct = len(correct)\n",
    "    correct_perc = num_correct/len(true_class)\n",
    "    num_pred = len(pred_class)\n",
    "    if num_pred == 0:\n",
    "        wrong_perc = math.nan\n",
    "    else:\n",
    "        wrong_perc = (num_pred - num_correct)/num_pred\n",
    "    df_perf = pd.DataFrame({\n",
    "    'accuracy': [accuracy],\n",
    "    'true_ident': [correct_perc],\n",
    "    'false_pred': [wrong_perc]\n",
    "    })\n",
    "    return(df_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10aa57d-d7ed-45c7-bdf5-29af193c7fd5",
   "metadata": {},
   "source": [
    "Provide a list of candidate models and apply them to the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd7869a8-b436-40de-9ea0-28eb4b7d3248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using model: joeddav/xlm-roberta-large-xnli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast convertors: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/huggingface_env/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:1496\u001b[0m, in \u001b[0;36mTikTokenConverter.extract_vocab_merges_from_model\u001b[0;34m(self, tiktoken_url)\u001b[0m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1496\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_tiktoken_bpe\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/huggingface_env/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:1633\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[1;32m   1632\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting from Tiktoken\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTikTokenConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huggingface_env/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:1533\u001b[0m, in \u001b[0;36mTikTokenConverter.converted\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tokenizer:\n\u001b[0;32m-> 1533\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1534\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer \u001b[38;5;241m=\u001b[39m pre_tokenizers\u001b[38;5;241m.\u001b[39mSequence(\n\u001b[1;32m   1535\u001b[0m         [\n\u001b[1;32m   1536\u001b[0m             pre_tokenizers\u001b[38;5;241m.\u001b[39mSplit(Regex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpattern), behavior\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misolated\u001b[39m\u001b[38;5;124m\"\u001b[39m, invert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1537\u001b[0m             pre_tokenizers\u001b[38;5;241m.\u001b[39mByteLevel(add_prefix_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space, use_regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1538\u001b[0m         ]\n\u001b[1;32m   1539\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huggingface_env/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:1526\u001b[0m, in \u001b[0;36mTikTokenConverter.tokenizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1526\u001b[0m     vocab_scores, merges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_vocab_merges_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1527\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(BPE(vocab_scores, merges, fuse_unk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huggingface_env/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:1498\u001b[0m, in \u001b[0;36mTikTokenConverter.extract_vocab_merges_from_model\u001b[0;34m(self, tiktoken_url)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1498\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tiktoken` is required to read a `tiktoken` file. Install it with \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tiktoken`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1500\u001b[0m     )\n\u001b[1;32m   1502\u001b[0m bpe_ranks \u001b[38;5;241m=\u001b[39m load_tiktoken_bpe(tiktoken_url)\n",
      "\u001b[0;31mValueError\u001b[0m: `tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m current_trip \u001b[38;5;241m=\u001b[39m trip_descriptions[i]\n\u001b[1;32m     24\u001b[0m current_type \u001b[38;5;241m=\u001b[39m trip_types[i]\n\u001b[0;32m---> 25\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpred_trip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_trip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcut_off\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# accuracy, perc true classes identified and perc wrong pred classes\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m, in \u001b[0;36mpred_trip\u001b[0;34m(model_name, trip_descr, trip_type, cut_off)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpred_trip\u001b[39m(model_name, trip_descr, trip_type, cut_off):\n\u001b[0;32m----> 6\u001b[0m     classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzero-shot-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Create an empty DataFrame with specified columns\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuperclass\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_class\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huggingface_env/lib/python3.8/site-packages/transformers/pipelines/__init__.py:1033\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m             tokenizer_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   1031\u001b[0m             tokenizer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1033\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_fast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_image_processor:\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huggingface_env/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:939\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    936\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 939\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huggingface_env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2213\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2211\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huggingface_env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2447\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2445\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2446\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2447\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2448\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[1;32m   2449\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2452\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huggingface_env/lib/python3.8/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py:108\u001b[0m, in \u001b[0;36mXLMRobertaTokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     94\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m ):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Mask token behave like a normal word, i.e. include the space before it\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     mask_token \u001b[38;5;241m=\u001b[39m AddedToken(mask_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43msep_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcls_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m vocab_file\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huggingface_env/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:138\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[0;32m--> 138\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huggingface_env/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:1638\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TikTokenConverter(\n\u001b[1;32m   1634\u001b[0m         vocab_file\u001b[38;5;241m=\u001b[39mtransformer_tokenizer\u001b[38;5;241m.\u001b[39mvocab_file,\n\u001b[1;32m   1635\u001b[0m         additional_special_tokens\u001b[38;5;241m=\u001b[39mtransformer_tokenizer\u001b[38;5;241m.\u001b[39madditional_special_tokens,\n\u001b[1;32m   1636\u001b[0m     )\u001b[38;5;241m.\u001b[39mconverted()\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1638\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1639\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1640\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1641\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently available slow->fast convertors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1642\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast convertors: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"
     ]
    }
   ],
   "source": [
    "# List of Hugging Face model names\n",
    "model_names = [\n",
    "    #\"facebook/bart-large-mnli\",\n",
    "    #\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\",\n",
    "    #\"cross-encoder/nli-deberta-v3-base\",\n",
    "    #\"cross-encoder/nli-deberta-v3-large\",\n",
    "    #\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\",\n",
    "    #\"joeddav/bart-large-mnli-yahoo-answers\",\n",
    "    #\"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\",\n",
    "    #\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\",\n",
    "    #\"valhalla/distilbart-mnli-12-1\",\n",
    "    #\"joeddav/xlm-roberta-large-xnli\" # keeps giving errors\n",
    "]\n",
    "\n",
    "# Apply each model to the test data\n",
    "for model_name in model_names:\n",
    "    print(f\"\\nUsing model: {model_name}\")\n",
    "    result_list = []\n",
    "    performance = pd.DataFrame(columns=['accuracy', 'true_ident', 'false_pred'])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i in range(len(trip_descriptions)):\n",
    "        current_trip = trip_descriptions[i]\n",
    "        current_type = trip_types[i]\n",
    "        df = pred_trip(model_name, current_trip, current_type, cut_off = 0.5)\n",
    "        print(df)\n",
    "        # accuracy, perc true classes identified and perc wrong pred classes\n",
    "        performance = pd.concat([performance, perf_measure(df)])\n",
    "        print(performance)\n",
    "        \n",
    "        result_list.append(df)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    # Extract \"same_value\" column from each DataFrame\n",
    "    sv_columns = [df['same_value'] for df in result_list]  # 'same' needs to be changed\n",
    "    sv_columns.insert(0, result_list[0]['superclass'])\n",
    "    # Combine into a new DataFrame (columns side-by-side)\n",
    "    sv_df = pd.concat(sv_columns, axis=1)\n",
    "    print(sv_df)\n",
    "    # Compute accuracy per superclass (row means of same_value matrix excluding the first column)\n",
    "    row_means = sv_df.iloc[:, 1:].mean(axis=1)\n",
    "    df_row_means = pd.DataFrame({\n",
    "        'superclass': sv_df['superclass'],\n",
    "        'accuracy': row_means\n",
    "    })\n",
    "    print(df_row_means)\n",
    "    # Compute performance measures per trip (mean for each column of performance table)\n",
    "    column_means = performance.mean()\n",
    "    print(column_means)\n",
    "    # save results\n",
    "    model = model_name.replace(\"/\", \"-\")\n",
    "    model_result = {\n",
    "        'model': model,\n",
    "        'predictions': result_list,\n",
    "        'performance': performance,\n",
    "        'perf_summary': column_means,\n",
    "        'perf_superclass': df_row_means,\n",
    "        'elapsed_time': elapsed_time\n",
    "    }\n",
    "    # File path with folder\n",
    "    filename = os.path.join('results', f'{model}_results.pkl')\n",
    "    # Save the object\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model_result, f)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cbb54e-abe6-49b6-957e-0683196f3199",
   "metadata": {},
   "source": [
    "**Load and compare results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62ca82b0-6909-4e6c-9d2c-fed87971e5b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: cross-encoder-nli-deberta-v3-base\n",
      "Performance Summary:\n",
      "accuracy      0.444444\n",
      "true_ident    0.533333\n",
      "false_pred    0.712500\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: joeddav-bart-large-mnli-yahoo-answers\n",
      "Performance Summary:\n",
      "accuracy      0.311111\n",
      "true_ident    0.650000\n",
      "false_pred    0.553792\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: cross-encoder-nli-deberta-v3-large\n",
      "Performance Summary:\n",
      "accuracy      0.466667\n",
      "true_ident    0.566667\n",
      "false_pred    0.541667\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: MoritzLaurer-DeBERTa-v3-large-mnli-fever-anli-ling-wanli\n",
      "Performance Summary:\n",
      "accuracy      0.566667\n",
      "true_ident    0.841667\n",
      "false_pred    0.546667\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: MoritzLaurer-mDeBERTa-v3-base-mnli-xnli\n",
      "Performance Summary:\n",
      "accuracy      0.466667\n",
      "true_ident    0.408333\n",
      "false_pred    0.481250\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: MoritzLaurer-deberta-v3-large-zeroshot-v2.0\n",
      "Performance Summary:\n",
      "accuracy      0.455556\n",
      "true_ident    0.325000\n",
      "false_pred    0.500000\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: facebook-bart-large-mnli\n",
      "Performance Summary:\n",
      "accuracy      0.466667\n",
      "true_ident    0.708333\n",
      "false_pred    0.400000\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: valhalla-distilbart-mnli-12-1\n",
      "Performance Summary:\n",
      "accuracy      0.522222\n",
      "true_ident    0.300000\n",
      "false_pred    0.533333\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: MoritzLaurer-DeBERTa-v3-base-mnli-fever-anli\n",
      "Performance Summary:\n",
      "accuracy      0.522222\n",
      "true_ident    0.841667\n",
      "false_pred    0.572381\n",
      "dtype: float64\n",
      "----------------------------------------\n",
      "Model: cross-encoder-nli-deberta-v3-base\n",
      "Performance Summary:\n",
      "           superclass  accuracy\n",
      "0       activity_type       0.7\n",
      "1          activities       0.0\n",
      "2   climate_or_season       0.4\n",
      "3    style_or_comfort       0.4\n",
      "4          dress_code       0.7\n",
      "5       accommodation       0.6\n",
      "6      transportation       0.6\n",
      "7  special_conditions       0.0\n",
      "8    trip_length_days       0.6\n",
      "----------------------------------------\n",
      "Model: joeddav-bart-large-mnli-yahoo-answers\n",
      "Performance Summary:\n",
      "           superclass  accuracy\n",
      "0       activity_type       0.7\n",
      "1          activities       0.1\n",
      "2   climate_or_season       0.5\n",
      "3    style_or_comfort       0.3\n",
      "4          dress_code       0.1\n",
      "5       accommodation       0.3\n",
      "6      transportation       0.3\n",
      "7  special_conditions       0.0\n",
      "8    trip_length_days       0.6\n",
      "----------------------------------------\n",
      "Model: cross-encoder-nli-deberta-v3-large\n",
      "Performance Summary:\n",
      "           superclass  accuracy\n",
      "0       activity_type       0.6\n",
      "1          activities       0.1\n",
      "2   climate_or_season       0.5\n",
      "3    style_or_comfort       0.4\n",
      "4          dress_code       0.7\n",
      "5       accommodation       0.7\n",
      "6      transportation       0.4\n",
      "7  special_conditions       0.3\n",
      "8    trip_length_days       0.6\n",
      "----------------------------------------\n",
      "Model: MoritzLaurer-DeBERTa-v3-large-mnli-fever-anli-ling-wanli\n",
      "Performance Summary:\n",
      "           superclass  accuracy\n",
      "0       activity_type       0.8\n",
      "1          activities       0.0\n",
      "2   climate_or_season       0.7\n",
      "3    style_or_comfort       0.4\n",
      "4          dress_code       0.7\n",
      "5       accommodation       0.8\n",
      "6      transportation       0.9\n",
      "7  special_conditions       0.2\n",
      "8    trip_length_days       0.6\n",
      "----------------------------------------\n",
      "Model: MoritzLaurer-mDeBERTa-v3-base-mnli-xnli\n",
      "Performance Summary:\n",
      "           superclass  accuracy\n",
      "0       activity_type       0.9\n",
      "1          activities       0.0\n",
      "2   climate_or_season       0.6\n",
      "3    style_or_comfort       0.5\n",
      "4          dress_code       0.6\n",
      "5       accommodation       0.3\n",
      "6      transportation       0.8\n",
      "7  special_conditions       0.1\n",
      "8    trip_length_days       0.4\n",
      "----------------------------------------\n",
      "Model: MoritzLaurer-deberta-v3-large-zeroshot-v2.0\n",
      "Performance Summary:\n",
      "           superclass  accuracy\n",
      "0       activity_type       0.7\n",
      "1          activities       0.0\n",
      "2   climate_or_season       0.5\n",
      "3    style_or_comfort       0.3\n",
      "4          dress_code       0.7\n",
      "5       accommodation       0.5\n",
      "6      transportation       0.8\n",
      "7  special_conditions       0.0\n",
      "8    trip_length_days       0.6\n",
      "----------------------------------------\n",
      "Model: facebook-bart-large-mnli\n",
      "Performance Summary:\n",
      "           superclass  accuracy\n",
      "0       activity_type       0.8\n",
      "1          activities       0.0\n",
      "2   climate_or_season       0.6\n",
      "3    style_or_comfort       0.4\n",
      "4          dress_code       0.7\n",
      "5       accommodation       0.3\n",
      "6      transportation       0.8\n",
      "7  special_conditions       0.0\n",
      "8    trip_length_days       0.6\n",
      "----------------------------------------\n",
      "Model: valhalla-distilbart-mnli-12-1\n",
      "Performance Summary:\n",
      "           superclass  accuracy\n",
      "0       activity_type       0.7\n",
      "1          activities       0.1\n",
      "2   climate_or_season       0.6\n",
      "3    style_or_comfort       0.4\n",
      "4          dress_code       0.7\n",
      "5       accommodation       0.4\n",
      "6      transportation       0.9\n",
      "7  special_conditions       0.3\n",
      "8    trip_length_days       0.7\n",
      "----------------------------------------\n",
      "Model: MoritzLaurer-DeBERTa-v3-base-mnli-fever-anli\n",
      "Performance Summary:\n",
      "           superclass  accuracy\n",
      "0       activity_type       0.8\n",
      "1          activities       0.0\n",
      "2   climate_or_season       0.5\n",
      "3    style_or_comfort       0.3\n",
      "4          dress_code       0.8\n",
      "5       accommodation       0.8\n",
      "6      transportation       0.7\n",
      "7  special_conditions       0.2\n",
      "8    trip_length_days       0.6\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Folder where .pkl files are saved\n",
    "results_dir = 'results'\n",
    "\n",
    "# Dictionary to store all loaded results\n",
    "all_results = {}\n",
    "\n",
    "# Loop through all .pkl files in the folder\n",
    "for filename in os.listdir(results_dir):\n",
    "    if filename.endswith('.pkl'):\n",
    "        model_name = filename.replace('_results.pkl', '')  # Extract model name\n",
    "        file_path = os.path.join(results_dir, filename)\n",
    "        \n",
    "        # Load the result\n",
    "        with open(file_path, 'rb') as f:\n",
    "            result = pickle.load(f)\n",
    "            all_results[model_name] = result\n",
    "\n",
    "# Compare performance across models\n",
    "for model, data in all_results.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Performance Summary:\\n{data['perf_summary']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Compare performance across models\n",
    "for model, data in all_results.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Performance Summary:\\n{data['perf_superclass']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f65e5b1-bc32-42c2-bbe9-9e3a6ffc72c1",
   "metadata": {},
   "source": [
    "**Identify trips that are difficult to predict**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040055c9-5df4-49b0-921a-5bf98ff01a69",
   "metadata": {},
   "source": [
    "Per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "57fd150d-1cda-4be5-806b-ef380469243a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross-encoder-nli-deberta-v3-base: Index([0, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')\n",
      "\n",
      "joeddav-bart-large-mnli-yahoo-answers: RangeIndex(start=0, stop=10, step=1)\n",
      "\n",
      "cross-encoder-nli-deberta-v3-large: Index([0, 1, 2, 3, 4, 6, 7, 8, 9], dtype='int64')\n",
      "\n",
      "MoritzLaurer-DeBERTa-v3-large-mnli-fever-anli-ling-wanli: Index([2, 5, 6, 7, 8, 9], dtype='int64')\n",
      "\n",
      "MoritzLaurer-mDeBERTa-v3-base-mnli-xnli: RangeIndex(start=0, stop=10, step=1)\n",
      "\n",
      "MoritzLaurer-deberta-v3-large-zeroshot-v2.0: Index([0, 1, 2, 3, 4, 5, 6, 7, 9], dtype='int64')\n",
      "\n",
      "facebook-bart-large-mnli: RangeIndex(start=0, stop=10, step=1)\n",
      "\n",
      "valhalla-distilbart-mnli-12-1: Index([0, 1, 2, 3, 4, 7, 9], dtype='int64')\n",
      "\n",
      "MoritzLaurer-DeBERTa-v3-base-mnli-fever-anli: Index([0, 2, 3, 4, 6, 7], dtype='int64')\n",
      "\n",
      "My partner and I are traveling to the Netherlands and Germany to spend Christmas with our family. We are in our late twenties and will start our journey with a two-hour flight to the Netherlands. From there, we will take a 5.5-hour train ride to northern Germany. \n",
      "\n",
      "We will go to Sweden in the winter, to go for a yoga and sauna/wellness retreat. I prefer lightweight packing and also want clothes to go for fancy dinners and maybe on a winter hike. We stay in hotels. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_difficult_trips(model_result, cut_off = 0.6):\n",
    "    # model_result is a dict with dict_keys(['model', 'predictions', \n",
    "    # 'performance', 'perf_summary', 'perf_superclass', 'elapsed_time'])\n",
    "    # get performance dataframe and repair index\n",
    "    df = model_result['performance'].reset_index(drop=True)\n",
    "    # find index of trips whose accuracy is below cut_off\n",
    "    index_result = df[df['accuracy'] < cut_off].index\n",
    "    return(index_result)\n",
    "\n",
    "# dictionary of trips that have accuracy below cut_off default\n",
    "difficult_trips_dict = {}\n",
    "for model, data in all_results.items():\n",
    "    difficult_trips_dict[data[\"model\"]] = get_difficult_trips(data)\n",
    "\n",
    "for key, value in difficult_trips_dict.items():\n",
    "    print(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91fb932-c5aa-472a-9b8d-a0cfc83a87f8",
   "metadata": {},
   "source": [
    "For all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a2754cb7-59b9-4f1d-ab74-1bf711b3eba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My partner and I are traveling to the Netherlands and Germany to spend Christmas with our family. We are in our late twenties and will start our journey with a two-hour flight to the Netherlands. From there, we will take a 5.5-hour train ride to northern Germany. \n",
      "\n",
      "We will go to Sweden in the winter, to go for a yoga and sauna/wellness retreat. I prefer lightweight packing and also want clothes to go for fancy dinners and maybe on a winter hike. We stay in hotels. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Which trips are difficult for all models\n",
    "common = set.intersection(*(set(v) for v in difficult_trips_dict.values()))\n",
    "for i in common:\n",
    "    print(trip_descriptions[i], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be58d66f-a491-4f47-98df-2c0aa4af38e7",
   "metadata": {},
   "source": [
    "**Identify superclasses that are difficult to predict**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e833c2d-9356-4d40-9b20-0a1eb6628a30",
   "metadata": {},
   "source": [
    "Per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "adb491b1-3ac3-4c32-934f-5eb6171f2ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           superclass  accuracy\n",
      "0       activity_type       0.7\n",
      "1          activities       0.0\n",
      "2   climate_or_season       0.4\n",
      "3    style_or_comfort       0.4\n",
      "4          dress_code       0.7\n",
      "5       accommodation       0.6\n",
      "6      transportation       0.6\n",
      "7  special_conditions       0.0\n",
      "8    trip_length_days       0.6\n",
      "           superclass  accuracy\n",
      "0       activity_type       0.7\n",
      "1          activities       0.1\n",
      "2   climate_or_season       0.5\n",
      "3    style_or_comfort       0.3\n",
      "4          dress_code       0.1\n",
      "5       accommodation       0.3\n",
      "6      transportation       0.3\n",
      "7  special_conditions       0.0\n",
      "8    trip_length_days       0.6\n",
      "           superclass  accuracy\n",
      "0       activity_type       0.6\n",
      "1          activities       0.1\n",
      "2   climate_or_season       0.5\n",
      "3    style_or_comfort       0.4\n",
      "4          dress_code       0.7\n",
      "5       accommodation       0.7\n",
      "6      transportation       0.4\n",
      "7  special_conditions       0.3\n",
      "8    trip_length_days       0.6\n",
      "           superclass  accuracy\n",
      "0       activity_type       0.8\n",
      "1          activities       0.0\n",
      "2   climate_or_season       0.7\n",
      "3    style_or_comfort       0.4\n",
      "4          dress_code       0.7\n",
      "5       accommodation       0.8\n",
      "6      transportation       0.9\n",
      "7  special_conditions       0.2\n",
      "8    trip_length_days       0.6\n",
      "           superclass  accuracy\n",
      "0       activity_type       0.9\n",
      "1          activities       0.0\n",
      "2   climate_or_season       0.6\n",
      "3    style_or_comfort       0.5\n",
      "4          dress_code       0.6\n",
      "5       accommodation       0.3\n",
      "6      transportation       0.8\n",
      "7  special_conditions       0.1\n",
      "8    trip_length_days       0.4\n",
      "           superclass  accuracy\n",
      "0       activity_type       0.7\n",
      "1          activities       0.0\n",
      "2   climate_or_season       0.5\n",
      "3    style_or_comfort       0.3\n",
      "4          dress_code       0.7\n",
      "5       accommodation       0.5\n",
      "6      transportation       0.8\n",
      "7  special_conditions       0.0\n",
      "8    trip_length_days       0.6\n",
      "           superclass  accuracy\n",
      "0       activity_type       0.8\n",
      "1          activities       0.0\n",
      "2   climate_or_season       0.6\n",
      "3    style_or_comfort       0.4\n",
      "4          dress_code       0.7\n",
      "5       accommodation       0.3\n",
      "6      transportation       0.8\n",
      "7  special_conditions       0.0\n",
      "8    trip_length_days       0.6\n",
      "           superclass  accuracy\n",
      "0       activity_type       0.7\n",
      "1          activities       0.1\n",
      "2   climate_or_season       0.6\n",
      "3    style_or_comfort       0.4\n",
      "4          dress_code       0.7\n",
      "5       accommodation       0.4\n",
      "6      transportation       0.9\n",
      "7  special_conditions       0.3\n",
      "8    trip_length_days       0.7\n",
      "           superclass  accuracy\n",
      "0       activity_type       0.8\n",
      "1          activities       0.0\n",
      "2   climate_or_season       0.5\n",
      "3    style_or_comfort       0.3\n",
      "4          dress_code       0.8\n",
      "5       accommodation       0.8\n",
      "6      transportation       0.7\n",
      "7  special_conditions       0.2\n",
      "8    trip_length_days       0.6\n",
      "cross-encoder-nli-deberta-v3-base: ['activities', 'climate_or_season', 'style_or_comfort', 'special_conditions']\n",
      "\n",
      "joeddav-bart-large-mnli-yahoo-answers: ['activities', 'style_or_comfort', 'dress_code', 'accommodation', 'transportation', 'special_conditions']\n",
      "\n",
      "cross-encoder-nli-deberta-v3-large: ['activities', 'style_or_comfort', 'transportation', 'special_conditions']\n",
      "\n",
      "MoritzLaurer-DeBERTa-v3-large-mnli-fever-anli-ling-wanli: ['activities', 'style_or_comfort', 'special_conditions']\n",
      "\n",
      "MoritzLaurer-mDeBERTa-v3-base-mnli-xnli: ['activities', 'accommodation', 'special_conditions', 'trip_length_days']\n",
      "\n",
      "MoritzLaurer-deberta-v3-large-zeroshot-v2.0: ['activities', 'style_or_comfort', 'special_conditions']\n",
      "\n",
      "facebook-bart-large-mnli: ['activities', 'style_or_comfort', 'accommodation', 'special_conditions']\n",
      "\n",
      "valhalla-distilbart-mnli-12-1: ['activities', 'style_or_comfort', 'accommodation', 'special_conditions']\n",
      "\n",
      "MoritzLaurer-DeBERTa-v3-base-mnli-fever-anli: ['activities', 'style_or_comfort', 'special_conditions']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_difficult_superclasses(model_result, cut_off = 0.5):\n",
    "    # model_result is a dict with dict_keys(['model', 'predictions', \n",
    "    # 'performance', 'perf_summary', 'perf_superclass', 'elapsed_time'])\n",
    "    df = model_result[\"perf_superclass\"]\n",
    "    # find superclass whose accuracy is below cut_off\n",
    "    diff_spc = list(df[df['accuracy'] < cut_off][\"superclass\"])\n",
    "    return(diff_spc)\n",
    "\n",
    "# dictionary of superclasses that have accuracy below cut_off default\n",
    "difficult_superclass_dict = {}\n",
    "for model, data in all_results.items():\n",
    "    difficult_superclass_dict[data[\"model\"]] = get_difficult_superclasses(data)\n",
    "\n",
    "for key, value in difficult_superclass_dict.items():\n",
    "    print(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcebdf8-0975-45cb-96f5-15b4645aa7f6",
   "metadata": {},
   "source": [
    "For all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4e51c11b-9a0a-4f9d-b20c-a6feda2d5a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activities', 'special_conditions'}\n"
     ]
    }
   ],
   "source": [
    "# Which trips are difficult for all models\n",
    "common = set.intersection(*(set(v) for v in difficult_superclass_dict.values()))\n",
    "print(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f0e31e2c-e87d-4776-b781-991919492430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'cross-encoder-nli-deberta-v3-base', 'predictions': [           superclass                                    pred_class  \\\n",
      "0       activity_type                micro-adventure / weekend trip   \n",
      "1          activities                                            []   \n",
      "2   climate_or_season            variable weather / spring / autumn   \n",
      "3    style_or_comfort                                    minimalist   \n",
      "4          dress_code                                        casual   \n",
      "5       accommodation                                        indoor   \n",
      "6      transportation                                no own vehicle   \n",
      "7  special_conditions  self-supported (bring your own food/cooking)   \n",
      "8    trip_length_days                                       7+ days   \n",
      "\n",
      "                                         true_class  same_value  \n",
      "0                                    beach vacation       False  \n",
      "1  [swimming, going to the beach, relaxing, hiking]       False  \n",
      "2                         warm destination / summer       False  \n",
      "3                     lightweight (but comfortable)       False  \n",
      "4                                            casual        True  \n",
      "5                                            indoor        True  \n",
      "6                                    no own vehicle        True  \n",
      "7                             no special conditions       False  \n",
      "8                                           7+ days        True  ,            superclass                            pred_class  \\\n",
      "0       activity_type                             city trip   \n",
      "1          activities  [relaxing, sightseeing, photography]   \n",
      "2   climate_or_season             warm destination / summer   \n",
      "3    style_or_comfort       luxury (including evening wear)   \n",
      "4          dress_code                                casual   \n",
      "5       accommodation                                indoor   \n",
      "6      transportation                        no own vehicle   \n",
      "7  special_conditions                        child-friendly   \n",
      "8    trip_length_days                                3 days   \n",
      "\n",
      "                           true_class  same_value  \n",
      "0                           city trip        True  \n",
      "1                       [sightseeing]       False  \n",
      "2  variable weather / spring / autumn       False  \n",
      "3     luxury (including evening wear)        True  \n",
      "4                              casual        True  \n",
      "5                              indoor        True  \n",
      "6                      no own vehicle        True  \n",
      "7                no special condition       False  \n",
      "8                              3 days        True  ,            superclass                                        pred_class  \\\n",
      "0       activity_type                                         city trip   \n",
      "1          activities  [photography, running, paragliding, sightseeing]   \n",
      "2   climate_or_season                variable weather / spring / autumn   \n",
      "3    style_or_comfort                                        minimalist   \n",
      "4          dress_code                                            casual   \n",
      "5       accommodation                                            indoor   \n",
      "6      transportation                                       own vehicle   \n",
      "7  special_conditions                                    child-friendly   \n",
      "8    trip_length_days                                           7+ days   \n",
      "\n",
      "                      true_class  same_value  \n",
      "0                      city trip        True  \n",
      "1                     [relaxing]       False  \n",
      "2      cold destination / winter       False  \n",
      "3  lightweight (but comfortable)       False  \n",
      "4                         casual        True  \n",
      "5                         indoor        True  \n",
      "6                 no own vehicle       False  \n",
      "7           no special condition       False  \n",
      "8                        7+ days        True  ,            superclass                                    pred_class  \\\n",
      "0       activity_type                long-distance hike / thru-hike   \n",
      "1          activities                         [sightseeing, hiking]   \n",
      "2   climate_or_season            variable weather / spring / autumn   \n",
      "3    style_or_comfort               luxury (including evening wear)   \n",
      "4          dress_code                                        casual   \n",
      "5       accommodation                          huts with half board   \n",
      "6      transportation                                   own vehicle   \n",
      "7  special_conditions  self-supported (bring your own food/cooking)   \n",
      "8    trip_length_days                                       7+ days   \n",
      "\n",
      "                           true_class  same_value  \n",
      "0                cultural exploration       False  \n",
      "1      [sightseeing, hiking, rafting]       False  \n",
      "2  variable weather / spring / autumn        True  \n",
      "3       lightweight (but comfortable)       False  \n",
      "4                              casual        True  \n",
      "5                              indoor       False  \n",
      "6                      no own vehicle       False  \n",
      "7                       rainy climate       False  \n",
      "8                             7+ days        True  ,            superclass                                         pred_class  \\\n",
      "0       activity_type                     micro-adventure / weekend trip   \n",
      "1          activities  [hiking, sightseeing, swimming, relaxing, phot...   \n",
      "2   climate_or_season                          warm destination / summer   \n",
      "3    style_or_comfort                      lightweight (but comfortable)   \n",
      "4          dress_code                                             casual   \n",
      "5       accommodation                               huts with half board   \n",
      "6      transportation                                        own vehicle   \n",
      "7  special_conditions                            avalanche-prone terrain   \n",
      "8    trip_length_days                                            7+ days   \n",
      "\n",
      "                      true_class  same_value  \n",
      "0                  nature escape       False  \n",
      "1   [swimming, relaxing, hiking]       False  \n",
      "2      warm destination / summer        True  \n",
      "3  lightweight (but comfortable)        True  \n",
      "4                         casual        True  \n",
      "5                         indoor       False  \n",
      "6                 no own vehicle       False  \n",
      "7          no special conditions       False  \n",
      "8                        7+ days        True  ,            superclass                                         pred_class  \\\n",
      "0       activity_type                     long-distance hike / thru-hike   \n",
      "1          activities  [hiking, going to the beach, sightseeing, hut-...   \n",
      "2   climate_or_season                 variable weather / spring / autumn   \n",
      "3    style_or_comfort                                         minimalist   \n",
      "4          dress_code                                             casual   \n",
      "5       accommodation                               huts with half board   \n",
      "6      transportation                                        own vehicle   \n",
      "7  special_conditions       self-supported (bring your own food/cooking)   \n",
      "8    trip_length_days                                             7 days   \n",
      "\n",
      "                       true_class  same_value  \n",
      "0  long-distance hike / thru-hike        True  \n",
      "1            [going to the beach]       False  \n",
      "2                tropical / humid       False  \n",
      "3                      minimalist        True  \n",
      "4                          casual        True  \n",
      "5            huts with half board        True  \n",
      "6                     own vehicle        True  \n",
      "7       off-grid / no electricity       False  \n",
      "8                          6 days       False  ,            superclass                          pred_class  \\\n",
      "0       activity_type                      beach vacation   \n",
      "1          activities                [going to the beach]   \n",
      "2   climate_or_season  variable weather / spring / autumn   \n",
      "3    style_or_comfort                          minimalist   \n",
      "4          dress_code                              casual   \n",
      "5       accommodation                  sleeping in a tent   \n",
      "6      transportation                      no own vehicle   \n",
      "7  special_conditions                      child-friendly   \n",
      "8    trip_length_days                             7+ days   \n",
      "\n",
      "                                 true_class  same_value  \n",
      "0                            beach vacation        True  \n",
      "1  [stand-up paddleboarding (SUP), surfing]       False  \n",
      "2                 cold destination / winter       False  \n",
      "3                                ultralight       False  \n",
      "4                                    casual        True  \n",
      "5                        sleeping in a tent        True  \n",
      "6                               own vehicle       False  \n",
      "7                 off-grid / no electricity       False  \n",
      "8                                    6 days       False  ,            superclass                             pred_class  \\\n",
      "0       activity_type                yoga / wellness retreat   \n",
      "1          activities  [yoga, relaxing, sightseeing, hiking]   \n",
      "2   climate_or_season     variable weather / spring / autumn   \n",
      "3    style_or_comfort        luxury (including evening wear)   \n",
      "4          dress_code                                 casual   \n",
      "5       accommodation                                 indoor   \n",
      "6      transportation                         no own vehicle   \n",
      "7  special_conditions                           snow and ice   \n",
      "8    trip_length_days                                7+ days   \n",
      "\n",
      "                      true_class  same_value  \n",
      "0        yoga / wellness retreat        True  \n",
      "1      [hut-to-hut hiking, yoga]       False  \n",
      "2      cold destination / winter       False  \n",
      "3  lightweight (but comfortable)       False  \n",
      "4         formal (business trip)       False  \n",
      "5             sleeping in a tent       False  \n",
      "6                 no own vehicle        True  \n",
      "7        avalanche-prone terrain       False  \n",
      "8                         7 days       False  ,            superclass                                         pred_class  \\\n",
      "0       activity_type                                 ski tour / skitour   \n",
      "1          activities  [ski touring, skiing, sightseeing, photography...   \n",
      "2   climate_or_season                          cold destination / winter   \n",
      "3    style_or_comfort                                         minimalist   \n",
      "4          dress_code                                             casual   \n",
      "5       accommodation                                             indoor   \n",
      "6      transportation                                     no own vehicle   \n",
      "7  special_conditions                                       snow and ice   \n",
      "8    trip_length_days                                            7+ days   \n",
      "\n",
      "                             true_class  same_value  \n",
      "0                    ski tour / skitour        True  \n",
      "1  [ski touring, photography, swimming]       False  \n",
      "2             cold destination / winter        True  \n",
      "3                            minimalist        True  \n",
      "4                          conservative       False  \n",
      "5                                indoor        True  \n",
      "6                        no own vehicle        True  \n",
      "7               avalanche-prone terrain       False  \n",
      "8                                5 days       False  ,            superclass                                         pred_class  \\\n",
      "0       activity_type                        camping trip (wild camping)   \n",
      "1          activities  [kayaking / canoeing, relaxing, swimming, snor...   \n",
      "2   climate_or_season                                   tropical / humid   \n",
      "3    style_or_comfort                                         minimalist   \n",
      "4          dress_code                                             casual   \n",
      "5       accommodation                                  sleeping in a car   \n",
      "6      transportation                                        own vehicle   \n",
      "7  special_conditions       self-supported (bring your own food/cooking)   \n",
      "8    trip_length_days                                             3 days   \n",
      "\n",
      "                            true_class  same_value  \n",
      "0          camping trip (wild camping)        True  \n",
      "1  [scuba diving, kayaking / canoeing]       False  \n",
      "2                     tropical / humid        True  \n",
      "3        lightweight (but comfortable)       False  \n",
      "4                         conservative       False  \n",
      "5                   sleeping in a tent       False  \n",
      "6                          own vehicle        True  \n",
      "7                no special conditions       False  \n",
      "8                               3 days        True  ], 'performance':    accuracy  true_ident  false_pred\n",
      "0  0.444444    0.000000         NaN\n",
      "0  0.666667    1.000000    0.666667\n",
      "0  0.444444    0.000000    1.000000\n",
      "0  0.333333    0.666667    0.000000\n",
      "0  0.444444    1.000000    0.812500\n",
      "0  0.555556    1.000000    0.833333\n",
      "0  0.333333    0.000000    1.000000\n",
      "0  0.222222    0.500000    0.750000\n",
      "0  0.555556    0.666667    0.600000\n",
      "0  0.444444    0.500000    0.750000, 'perf_summary': accuracy      0.444444\n",
      "true_ident    0.533333\n",
      "false_pred    0.712500\n",
      "dtype: float64, 'perf_superclass':            superclass  accuracy\n",
      "0       activity_type       0.7\n",
      "1          activities       0.0\n",
      "2   climate_or_season       0.4\n",
      "3    style_or_comfort       0.4\n",
      "4          dress_code       0.7\n",
      "5       accommodation       0.6\n",
      "6      transportation       0.6\n",
      "7  special_conditions       0.0\n",
      "8    trip_length_days       0.6, 'elapsed_time': 593.8561627864838}\n"
     ]
    }
   ],
   "source": [
    "print(all_results[\"cross-encoder-nli-deberta-v3-base\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e24355-4aac-4ad6-b50c-96f75585ce45",
   "metadata": {},
   "source": [
    "**Comparing models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b020f584-1468-4c84-9dac-7ca7fac6e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make table of 'perf_summary' for all models inlcude time elapsed\n",
    "# Make ranking from that table for each category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17483df4-55c4-41cd-b8a9-61f7a5c7e8a3",
   "metadata": {},
   "source": [
    "**Use gradio for user input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cb7fd425-d0d6-458d-97ca-2150dc55f206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://aa06d5d85ffadaa92b.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://aa06d5d85ffadaa92b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# use model with gradio\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "# make a function for what I am doing\n",
    "def classify(text):\n",
    "    df = pd.DataFrame(columns=['Superclass', 'class'])\n",
    "    for i, key in enumerate(keys_list):\n",
    "        # Run the classification (ca 30 seconds classifying)\n",
    "        if key == 'activities':\n",
    "            result = classifier(text, candidate_labels[key], multi_label=True)\n",
    "            classes = [result['labels'][i] for i in indices]\n",
    "        else:\n",
    "            result = classifier(text, candidate_labels[key])\n",
    "            classes = result[\"labels\"][0]\n",
    "        print(i)\n",
    "        df.loc[i] = [key, classes]\n",
    "\n",
    "    return df\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=classify,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"dataframe\",\n",
    "    title=\"Zero-Shot Classification\",\n",
    "    description=\"Enter a text describing your trip\",\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e856a9c-a66c-4c4b-b7cf-8c52abbbc6fa",
   "metadata": {},
   "source": [
    "Use model with gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "521d9118-b59d-4cc6-b637-20202eaf8f33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://0f70ba5369d721cf8f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0f70ba5369d721cf8f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the Gradio interface\n",
    "def classify(text):\n",
    "    return classifier(text, class_labels)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=classify,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"json\",\n",
    "    title=\"Zero-Shot Classification\",\n",
    "    description=\"Enter a text describing your trip\",\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8da1c90-d3a3-4b08-801c-b3afa17b2633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (huggingface_env)",
   "language": "python",
   "name": "huggingface_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
